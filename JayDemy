Q 01: An Nginx Deploy named nginx-static is Running in the nginx-static NS, It is configured using a cfgmap named nginx-config. update the nginx-config cfgMap
to allow only TLSv1.3 connections.re-create , restart or scale resources as necessary, By using command to test the changes:

Question 02: Migrate an existing web application from ingress to Gateway API, We must maintain HTTPSaccess.
A Gatewayclass named nginx is installed in the cluster.
First , create a gateway named web-gateway with hostname gateway.web.k8s.local that maintain the existing TLS and listeners configuration from the existing 
ingress resource named web.
Next, create an HTTPRoute named web-route with hostname gateway.web.k8s.local that maintains the existing routing rules from the current ingress resource 
named web.  

question 03: Create a new Horizontal POD Autoscaler( HPA) named apache-server in the autoscale namespace, This HPA must target the existing Depoyment called 
apache-server in the autoscale namespace.  
set the HPA to atrget for 50% CPU usage per pod, Configure hpa to have at min 1 pod and no more than 4 pods[max]. Also we have to set the downscale 
stabilization window to 30 seconds.  

Question 04: Create a new Ingress resources echo in echo-sound namespace EXposing Service echoserver-service on http://example.org/echo using Service 
port 8080 The availability of service echoserver-service can be checked, using the following commad, which should return 200:


question 05: Install and configure a container Network interface (CNI) of your choice that meets the specified requirements, choose one of the following
CNI options:
Falnnel (v0.26.1) [kube-flannel.yml]  
Calico (V3.28.2) using the manifest: [tigera-opeator.yaml]   
Ensure the selected CNI is properly installed and configured in the kubernetes cluster.  

Question 06: Install and configure a container networl interface (CNI) of your choice that meets the specified reqirements. choose one of the following 
CNI options: Falnnel using the manifest https://github/flannel  
calico using the manifest: https://rwa.github  
Ensure the selcted CNI is properly installed and configures in the kubernetes cluster.  
The CNI you choose must:  Let pods communicate with each other support Network policy enforcemet Install from manifest files ( do not use Helm)  

Question 07: Install ArgoCD in cluster: Add the official Argo CD Helm repository with the name argo. The Argo CD CRDs have already been pre-installed in the
cluster , generate a helm template of the Argo CD Helm chart version 7.7.3 for the argocd NS and save to /argo-helm.yaml confiure the chart to not install CRDs.
Install Argo CD using Helm with release name argocd using the same version as above and configuration as use in the template 7.7.3 install it in the argocd ns 
and configure it to not install CRDs. you dont need to access the Argo CD server UI.  

Question 08: Create a new PriorityClass named high-priority for user workloads witha value that is one less than the highest existing user-defined priority 
class value. Patch the existing Deployment busybox-logger running in the priority namespace to use the high-priority class. Ensure that the busybox-logger 
Deployment rolls out successfully with the new priority class set, It is expected that pods from other Deployments running in the priority namespace are evicted.
Dont modify other Deployments running in the priority namespace. Failure to do so may result in a reduced score.  

Question 09: Reconfigure the existing Deployment front-end in namespace sp-culator to expose port 80/tcp of the existing container nginx. Create a new Service
named front-end-svc exposing the container port 80/tcp. Configure the new Service to also expose the individual pods via & NodePort.  

Question 10: Create a new Storageclass named low-latency that uses the existing provisioner rancer.io/loca-path. 
Set the VolumeBinding Mode to Waitfor firstconsumer. (Mandatory or the score will be reduce) , Make the newly created StorageClass (low-latency) the default
Storage class in the cluster. DO NOT modify any existing Deployments or PersistentVolumeClaims (if modified, the score will be reduced).  

Question 11: A legacy app needs to be integrated into the kubernetes built-in logging architecture(i.e kubectl logs). Adding a stream co-located container is 
a good common way to accomplish this requirements.  
##TASK:## Update the existing Deployment synergy-deployment ,adding a co-located conatainer named sidecar using the busybox:stable image to the existing pod.
The new co-located container has to run the following command: 
## /bin/sh-c "tail -n+1 -f /var/log/synergy-deployment.log" ##
Use a vloume mounted at /var/log to make the log file synergy-deployment.log availabe to the co located conatainer, don't modify the specification of the 
existing conatiner other than adding the required, HINT: Use a shared volume to expose the log file B/W the main app conainer & the sidecar.  

Question 12: Verify the cert-manager application which has been deployed in the cluster, create a list of all cert-manager custom resource definations (CRDs)
and save it ot ~/resources.yaml. make sure kubectl default output format and use kubectl to list CRDs'. DO not set an output format.  
failure to do so will result in a reduced score. Using Kubectl, extract the document for the subject specification filed of the certification custom resource 
and save it to ~/subject.yaml, you may use any output format that kubcl supports.  

Question 13: A wordpress app with 3 replicas in the relative-fawn namespace consist of CPU 1 memory 2015360ki , Adjust all pod resource request as follows:
Divide node resources evenly across all 3 pods. give each pod a fair share of cpu and memory, add enough overhead to keep the node stable, use the exact same
request for both containers and init containers you are not required to change any resource limits, 
it may help to temporarily scale the wordpress deployment to 0 replicas while updating the resource requests, afte update , confirms.
wordpress keep 3 replicas && all pods are running and ready.  

Question14: A user accidentally delted the mariaDB Deployment in the mariadb namespace, which was configured with persistent storage. 
your responsibility is to re-establish the deployment while ensuring date is preserved by resusing the available Persistent Volume.
Task: A persistent Volume already exists and is retained for reuse. only one PV exists.
Create a persistentVolume Claim (PVC) named mariadb in the mariadb NS
with the spec: 
Acess mode READWriteOnce and storage 250MI, edit the mariadb deploy file located at ~/mariadb-deploy.yaml to use PVC created in the previous step.
Apply the updated Deployment file to the cluster, Ensure Mariadb is running and stable.  

Question 15: Prepare a linux system for kubernetes, Docker is already installed, but you need to configure it for kubeadm.
Task: complete these tasks to prepare the system for kubernetes: Set uo cri-dockerd: Install the debian package: ~/cri-dockerd_0.3.9.3-0.ubuntu-jammy_amd64.deb
Debian packages are installed using dpkg Enable and start the CRI-docker service Configure these system parameteres: 
Set net.ipv6.conf.all.forwarding to 1  Set net.ipv4.ip_forward to 1  Set net.netfilter.nf_conntrack_max to 131072  

Question 16: we have frontend and backend deploy in seprate NS (frontend and backend), They need to communicate. 
Analyze: Inspect the frontend and backend Deployments t understand their communication requirements.
Apply: From the NetworkPolicy YAML files in the ~/netpol folder, choose one to apply . IT must:
Allow communication b/w frontend and backend , be as restrictive as possible(least permissive)
Do not delte or cange the existing "deny-all" netpol's. Failure to follow these rules may result in a score reduction or zero.  

Question 17: A kubeadm provisioned cluster was migrated to a new machine, requires configuration change to run successfully.
Task: we need fix a single-node cluster that got broken during machine migration. Identify the broken cluster components and investigate what caused to break
those components, The decommissioned cluster used an external etcd server. Next,fix the configuration of all broken cluster components. 
Ensure to restart all necessary service and components for change to take effect, Finally ensure the cluster , single node and all pods are Ready.  


IT kiddie and jayDemy ques 

1> Argocd same 
2 > sidecar also same
3> migrate application is same  Check this ques in udemy trickey ques.
4> create a new storage class is same
5> Create a new PriorityClass named high-priority  same 
6>  Create a new Ingress resources echo in echo-sound same 

















